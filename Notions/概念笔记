## 卷积$h=f*g$：

**2元函数**运算，表示h取特定值t时，f,g变量之和为t，二者相乘，所有情况的累加/积分

$h(t)=\sum_{i+j=t}f(i)g(j)$或$h(t)=\int_{-\infin}^{+\infin}f(x)g(t-x)\mathrm{d}x$ 

离散情况下，将f,g的所有取值列成二维表格，卷积h的每一项表示为**次对角线方向所有项之和**

## SoftMax函数

一种激活函数，Sigmoid的扩展，用于多分类问题。

输入一个向量$z$，输出一个概率向量$p$，输出满足：

- 对应的输入越大，输出就越大
- 输出各项为0~1，和为1，因此可以对有限项的离散概率分布梯度做对数归一化，得到属于对应分类的概率
- 输入为负数时结果为0，此权重梯度为0，不会被更新而“死亡”

这个函数经常用于NN的最后一层，输出分类的概率

![image.png](attachment:91b99478-4276-4e91-a587-a07636a90720:image.png)

## 残差连接(**skip connect / residual connections)**

又称为短路连接(shortcut connect)，将模型学习到的输出视作和输入值的残差。

简单来说，就是在某一层的输出结果上加上输入。即使该层完全没有发挥学习的效果（输出梯度=0），至少结果不会比去掉该层更糟糕（加上输入本身的梯度1），能够很好地抑制模型退化、梯度爆炸和消失问题

残差链接是当前深度神经网络的基础，让数千层甚至更深的网络成为可能

## 消融实验(ablation study)

AI领域重要的研究方法：去除AI系统中的一个部分，分析该部分对系统的贡献。

分析部分是重要（去除后性能极大下降）、不重要（性能下降小）还是设计缺陷（去除后性能反而上升）

**可去除的部分/模块/选项：去除来设计实验**

**不可去除的变量/超参数等：采用grid search等方法来展示参数对于系统的影响**

核心：**单一变量**（单尺度叙事）原则，**鲁棒性**（去除部分仍然可以运行）

## Epoch Batch Step/Iteration

- Epoch：模型对所有数据进行一次训练
- Batch：将总体数据分成的批次
- Step：模型对一个批次进行一次训练

## 数据增强(data augmentation)

> 可以在自己项目使用的数据增强方法：https://github.com/aleju/imgaug
> 

在数据量不够时，通过**处理已有数据**进行处理，增加训练数据集量

图像处理领域的常见操作：

- 单样本：
    - 几何变换：**随机的裁剪，旋转，拉伸（通常效果更差），移位等**
    - 颜色变换：增加噪声、颜色扰动等
- 多样本：
    - SMOTE：使用插值，将小样本合成为新的样本，使得大小样本量平衡
        
        ![image.png](attachment:0090b176-461e-4934-bbd6-f3ed9f2d273b:image.png)
        
    - SamplePairing：**随机**抽取两张图片，**单样本处理**后对像素取平均值
    对医学图像比较有效
    - mixup：对GAN很有效
        
        ![image.png](attachment:7cc6aea6-3b31-4535-a7a5-a9eded5204ae:image.png)
        
- 无监督：利用无监督学习，学习数据分布，生成新图片或得到数据增强方法
    - GAN
    - **Autoaugmentation**
