以当下的眼光重新审视AlexNet，分析其划时代意义和有些落后的地方。

## 划时代意义

1. 端到端：不对输入做任何处理，直接利用（尽管原文做了裁剪，未强调其作用）
2. 提示增加网络深度可以提高模型性能，揭开了深度学习的序幕

## 落后

1. 正则化：AlexNet强调正则化对防止过拟合的作用，现在认为**更重要是网络设计**
2. Introduction：不仅要提自己的方法，还要在较宽的视野介绍他人的
3. 模型分割：限于当时的GPU性能，AlexNet的网络分到了2个GPU，现在来说不重要
**然而，或许可以参考用于更巨大的模型？**
4. ReLU：现在看来过度强调了其运算速度，但仍然因为便捷而大量使用
5. 原地Normalization：现在几乎不用，多用Batch Norm或Layer Norm
6. DropOut：每次以0.5概率杀掉部分神经元，使得模型多样化防止过拟合。现在用处不大，约等于L2正则项，某些模型仍然会使用
7. 学习率调整：过去的模型会根据训练效果手动下降学习率，或者每xx个epoch下降，现在通常采用cos等平滑的曲线
